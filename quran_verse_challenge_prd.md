# Qurâ€™anÂ VerseÂ ChallengeÂ â€” Product Requirements Document

_VersionÂ 0.1 Â | Â Last updated:Â 2025â€‘08â€‘07_

---

## 1Â â€¯Overview

A gamified SaaS that helps users memorise and understand Qurâ€™anic verses through daily AIâ€‘generated quizzes, recitation challenges, and scholarâ€‘verified explanations. Built as an **AIâ€‘native** product, it relies on a network of ClaudeÂ Code agents that collaborate (in parallel where possible) to automate content generation, code delivery, QA, and deployment.

---

## 2Â â€¯Goals &Â SuccessÂ Metrics

| Objective        | KPI                                 | Target @Â MonthÂ 6 |
| ---------------- | ----------------------------------- | ---------------- |
| User acquisition | Registered learners                 | â‰¥Â 10â€¯k           |
| Engagement       | AvgÂ DAUÂ /Â MAU                       | â‰¥Â 40Â %           |
| Learning outcome | Verseâ€‘recall accuracy after 30Â days | â‰¥Â 70Â %           |
| Revenue          | MRR                                 | â‰¥Â USÂ \$5â€¯k       |

---

## 3Â â€¯Personas

1. **Everyday Learner** â€“ Collegeâ€‘age Muslim aiming to finish 30Â juzâ€™ memorisation.
2. **Madrasah Teacher** â€“ Needs readyâ€‘made, reliable quizzes for class.
3. **Revert Muslim** â€“ Wants structured entryâ€‘level guidance.

---

## 4Â â€¯Scope

### 4.1Â MVP (firstÂ 3Â months)

- Verse Quiz Engine (MCQ &Â fillâ€‘inâ€‘theâ€‘blank)
- Daily Challenge &Â Streaks
- Progress Dashboard (perÂ juzâ€™, surah, topic)
- Scholar Moderation Queue

### 4.2Â PhaseÂ 2 (monthsÂ 4â€‘9)

- Speechâ€‘toâ€‘Recitation Scoring
- Spacedâ€‘Repetition Review Stack
- Group Mode / Leaderboards

---

## 5Â â€¯FunctionalÂ Requirements

| #      | Requirement                                                      | Acceptance Criteria                                    |
| ------ | ---------------------------------------------------------------- | ------------------------------------------------------ |
| Â FRâ€‘01 | System shall present a 5â€‘question daily quiz by 04:00 local time | Quiz pulls only approved questions; resets every 24Â h  |
| Â FRâ€‘02 | Users shall resume incomplete quizzes                            | State saved in `attempts` â‰¤Â 15Â s after last action     |
| Â FRâ€‘03 | Teacher accounts can assign quizzes to a group                   | Assignment visible to all group members within 1Â min   |
| Â FRâ€‘04 | Scholar approval is mandatory before a question becomes live     | Questions without `approved_at` timestamp never served |

---

## 6Â â€¯Nonâ€‘FunctionalÂ Requirements

- **Security**Â â€“ PDPA/GDPR compliant; rowâ€‘level security on Supabase.
- **Performance**Â â€“ P95 API latencyÂ <Â 300Â ms.
- **Accessibility**Â â€“ WCAGÂ 2.1Â AA.
- **Cost**Â â€“ Infra spend â‰¤Â 15Â % of MRR at scale of 50â€¯k MAU.

---

## 7Â â€¯Agentâ€‘Based Delivery Model (ClaudeÂ Code)

Below is how we decompose work into _main agents_ and _subâ€‘agents_ that can run concurrently.

```mermaid
flowchart TD
  PM["ðŸ§­Â Productâ€‘ManagerÂ Agent"]
  Backend["ðŸ”§Â BackendÂ Agent"]
  Frontend["ðŸŽ¨Â FrontendÂ Agent"]
  AIQ["ðŸ¤–Â AIâ€‘QuestionÂ Agent"]
  Scholar["ðŸ“œÂ Scholarâ€‘ValidationÂ Agent"]
  QA["âœ…Â QAÂ Agent"]
  DevOps["ðŸš€Â DevOpsÂ Agent"]

  PM -->|Stories &Â Specs| Backend & Frontend & AIQ & QA & DevOps
  AIQ --> Scholar
  Scholar --> Backend["ðŸ”§Â BackendÂ Agent"]
  Backend --> QA
  Frontend --> QA
  QA --> DevOps
```

### 7.1Â Main Agents &Â Roles

| Agent                  | Responsibility                                 | Parallelâ€‘safe Subâ€‘Agents                         |
| ---------------------- | ---------------------------------------------- | ------------------------------------------------ |
| **Productâ€‘Manager**    | Break epics â†’ userÂ stories; prioritise backlog | _Roadmapâ€‘Writer_, _Metricsâ€‘Tracker_              |
| **Backend**            | DB schema, API routes, RLS policies            | _Schemaâ€‘Designer_, _APIâ€‘Coder_, _Testâ€‘Writer_    |
| **Frontend**           | Next.js pages, state mgmt, Tailwind UI         | _Componentâ€‘Builder_, _Storybookâ€‘Doc_, _E2Eâ€‘Test_ |
| **AIâ€‘Question**        | Generate &Â embed quiz items; tag by topic      | _Verseâ€‘Fetcher_, _Distractorâ€‘Generator_          |
| **Scholarâ€‘Validation** | Humanâ€‘inâ€‘theâ€‘loop moderation queue             | _Arabicâ€‘Proofreader_, _Tafsirâ€‘Checker_           |
| **QA**                 | Unit, integration, recitationâ€‘audio tests      | _Loadâ€‘Tester_, _Accessibilityâ€‘Auditor_           |
| **DevOps**             | CI/CD, Vercel deploy, cost alerts              | _Pipelineâ€‘Maintainer_, _Infraâ€‘Guard_             |

_All agents share a Redis taskÂ queue so subâ€‘agents can work independently without blocking siblings._

---

## 8Â â€¯DataÂ Model (MVP)

```sql
users(id, email, role, created_at)
verses(id, surah, ayah, arabic_text, translation_en)
questions(id, verse_id, prompt, choices[], answer, difficulty, approved_at)
attempts(id, user_id, question_id, correct, answered_at)
streaks(user_id, current_streak, longest_streak, updated_at)
```

---

## 9Â â€¯Integration Points

| Service        | Purpose                            | AuthÂ Method                         |
| -------------- | ---------------------------------- | ----------------------------------- |
| OpenAIÂ GPTâ€‘4o  | Quiz generation, embeddings search | EnvÂ var API key                     |
| OpenAIÂ Whisper | Recitation scoring (PhaseÂ 2)       | Presigned S3Â upload â†’ serverless fn |
| PostHog        | Product analytics                  | ServerÂ key via env                  |
| Stripe         | Subscription billing               | OAuth Connect                       |

---

## 10Â â€¯Milestones &Â SprintÂ Plan

| SprintÂ #   | Duration | Deliverables                                |
| ---------- | -------- | ------------------------------------------- |
| Â 0Â (Setup) | 1Â wk     | Repo, Vercel, Supabase project, CI pipeline |
| Â 1         | 2Â wks    | DB schema, auth flow, verse seed script     |
| Â 2         | 2Â wks    | Quiz API, basic Next.js pages               |
| Â 3         | 2Â wks    | Scholar moderation UI, email invites        |
| Â 4         | 2Â wks    | Daily challenge cron, progress dashboard    |
| Â 5         | 2Â wks    | QA hardening, load test, open beta          |
| Â 6         | 2Â wks    | Payments, public launch                     |

---

## 11Â â€¯Risks &Â Mitigations

| Risk                          | Impact                     | Mitigation                                             |
| ----------------------------- | -------------------------- | ------------------------------------------------------ |
| AI hallucination in questions | Users learn incorrect info | Mandatory scholar approval; automated Arabic text diff |
| Cost overrun on GPT tokens    | Margin squeeze             | Fineâ€‘tune lighter model byÂ MonthÂ 6                     |
| Low teacher adoption          | Slower growth              | Early coâ€‘design sessions with 3 pilot madrasahs        |

---

## 12Â â€¯OpenÂ Questions

1. Which official Qurâ€™an translation(s) do we licence for English text?
2. Should recitation scoring be limited to tajwid basics or full phonemeâ€‘level accuracy?
3. Charity leaderboard: flat donation conversion or percentage of subscription revenue?

---

_End ofÂ PRD_
